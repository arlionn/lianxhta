# Hugging Face åœ¨çº¿è¯¾ç¨‹ç³»åˆ—ï¼šLLM Course

> ã€Œè¿žäº«ä¼šã€æŽ¨æ–‡ï¼šä»Ž Transformer åˆ°å¤§è¯­è¨€æ¨¡åž‹ï¼Œç³»ç»ŸæŽŒæ¡ NLP æ ¸å¿ƒæŠ€æœ¯

## 1. å¼•è¨€

å¤§è¯­è¨€æ¨¡åž‹ (Large Language Models, LLMs) æ­£åœ¨é‡å¡‘äººå·¥æ™ºèƒ½çš„åº”ç”¨è¾¹ç•Œã€‚ä»Ž ChatGPT åˆ° Llamaï¼Œä»Žæ–‡æœ¬ç”Ÿæˆåˆ°å¤šæ¨¡æ€ç†è§£ï¼ŒLLM å·²ç»æˆä¸ºå½“ä»Š AI é¢†åŸŸæœ€å…·å½±å“åŠ›çš„æŠ€æœ¯ã€‚ç„¶è€Œï¼Œè¦çœŸæ­£æŽŒæ¡ LLM çš„åŽŸç†ã€è®­ç»ƒå’Œåº”ç”¨ï¼Œéœ€è¦ç³»ç»Ÿçš„å­¦ä¹ è·¯å¾„å’Œå®žè·µç»éªŒã€‚

Hugging Face æŽ¨å‡ºçš„ [LLM Course](https://huggingface.co/learn/llm-course) æ­£æ˜¯è¿™æ ·ä¸€é—¨å…¨é¢è€Œæ·±å…¥çš„è¯¾ç¨‹ã€‚ä½œä¸ºæ›¾ç»çš„ã€ŒNLP Courseã€çš„å…¨é¢å‡çº§ç‰ˆï¼Œè¿™é—¨è¯¾ç¨‹ä¸ä»…ä¿ç•™äº† Transformer æ¨¡åž‹çš„ç»å…¸å†…å®¹ï¼Œæ›´å¢žåŠ äº†å¤§è¯­è¨€æ¨¡åž‹å¾®è°ƒã€æŽ¨ç†ä¼˜åŒ–ã€æŽ¨ç†æ¨¡åž‹æž„å»ºç­‰å‰æ²¿ä¸»é¢˜ã€‚å®Œå…¨å…è´¹ã€æ— å¹¿å‘Šï¼Œæ˜¯å­¦ä¹ çŽ°ä»£ NLP å’Œ LLM æŠ€æœ¯çš„ç†æƒ³é€‰æ‹©ã€‚

**[å›¾ 1ï¼šLLM Course è¯¾ç¨‹æž¶æž„å›¾]**
*è¯´æ˜Žï¼šå±•ç¤ºè¯¾ç¨‹çš„æ•´ä½“ç»“æž„å’Œå­¦ä¹ è·¯å¾„*

## 2. ä»Ž NLP åˆ° LLMï¼šè¯¾ç¨‹çš„æ¼”è¿›

### 2.1 ä¸ºä»€ä¹ˆä»Ž NLP Course å‡çº§ä¸º LLM Courseï¼Ÿ

Hugging Face çš„ NLP Course åœ¨è¿‡åŽ»ä¸‰å¹´ä¸­ä¸€ç›´æ˜¯å¼€æº AI ç¤¾åŒºçš„é‡è¦å­¦ä¹ èµ„æºã€‚ä½†éšç€ AI é¢†åŸŸçš„å¿«é€Ÿå‘å±•ï¼Œç‰¹åˆ«æ˜¯å¤§è¯­è¨€æ¨¡åž‹çš„çªç ´æ€§è¿›å±•ï¼ŒåŽŸæœ‰çš„è¯¾ç¨‹åç§°å·²æ— æ³•å‡†ç¡®åæ˜ å…¶å†…å®¹èŒƒå›´ã€‚æ–°å¢žçš„ç« èŠ‚æ¶µç›–äº† LLM å¾®è°ƒã€æŽ¨ç†æ¨¡åž‹æž„å»ºç­‰ä¸»é¢˜ï¼Œè¿™äº›éƒ½è¶…è¶Šäº†ä¼ ç»Ÿ NLP çš„èŒƒç•´ã€‚

**NLP vs. LLMsï¼šæ ¸å¿ƒåŒºåˆ«**

- **è‡ªç„¶è¯­è¨€å¤„ç† (NLP)**ï¼šå…³æ³¨è®©è®¡ç®—æœºç†è§£ã€è§£é‡Šå’Œç”Ÿæˆäººç±»è¯­è¨€çš„å¹¿æ³›é¢†åŸŸï¼ŒåŒ…æ‹¬æƒ…æ„Ÿåˆ†æžã€å‘½åå®žä½“è¯†åˆ«ã€æœºå™¨ç¿»è¯‘ç­‰ä¼ ç»Ÿä»»åŠ¡

- **å¤§è¯­è¨€æ¨¡åž‹ (LLMs)**ï¼šNLP é¢†åŸŸçš„å¼ºå¤§å­é›†ï¼Œä»¥æµ·é‡å‚æ•°ã€å¤§è§„æ¨¡è®­ç»ƒæ•°æ®å’Œé›¶æ ·æœ¬/å°‘æ ·æœ¬å­¦ä¹ èƒ½åŠ›ä¸ºç‰¹å¾ã€‚ä»£è¡¨æ€§æ¨¡åž‹åŒ…æ‹¬ GPT ç³»åˆ—ã€Llama ç³»åˆ—ã€Claude ç³»åˆ—ç­‰

è¯¾ç¨‹å¼ºè°ƒï¼šç†è§£ NLP åŸºç¡€å¯¹äºŽæœ‰æ•ˆä½¿ç”¨ LLM è‡³å…³é‡è¦ã€‚å› æ­¤ï¼ŒLLM Course æ—¢åŒ…å«ä¼ ç»Ÿ NLP æ¦‚å¿µï¼Œä¹Ÿæ¶µç›–å‰æ²¿çš„ LLM æŠ€æœ¯ã€‚

**[å›¾ 2ï¼šNLP åˆ° LLM çš„æŠ€æœ¯æ¼”è¿›]**
*è¯´æ˜Žï¼šå±•ç¤ºä»Žä¼ ç»Ÿ NLP æ–¹æ³•åˆ°çŽ°ä»£ LLM çš„å‘å±•åŽ†ç¨‹*

## 3. è¯¾ç¨‹å†…å®¹ï¼šä¸‰å¤§éƒ¨åˆ†ï¼ŒåäºŒä¸ªç« èŠ‚

### 3.1 ç¬¬ä¸€éƒ¨åˆ†ï¼šTransformer åŸºç¡€ (Chapters 1-4)

è¿™ä¸€éƒ¨åˆ†èšç„¦äºŽ ðŸ¤— Transformers åº“çš„æ ¸å¿ƒæ¦‚å¿µï¼Œä¸ºåŽç»­å­¦ä¹ å¥ å®šåšå®žåŸºç¡€ã€‚

**Chapter 1: Transformer æ¨¡åž‹åŸºç¡€**

- **Transformer èƒ½åšä»€ä¹ˆ**ï¼šé€šè¿‡ `pipeline()` å‡½æ•°å¿«é€Ÿä½“éªŒæ–‡æœ¬åˆ†ç±»ã€é—®ç­”ã€æ–‡æœ¬ç”Ÿæˆç­‰ä»»åŠ¡
- **å·¥ä½œåŽŸç†æ·±åº¦è§£æž**ï¼šæ³¨æ„åŠ›æœºåˆ¶ (Attention)ã€ç¼–ç å™¨-è§£ç å™¨æž¶æž„
- **æž¶æž„å˜ä½“**ï¼šç¼–ç å™¨æ¨¡åž‹ (BERT)ã€è§£ç å™¨æ¨¡åž‹ (GPT)ã€ç¼–ç å™¨-è§£ç å™¨æ¨¡åž‹ (T5)
- **æ¨¡åž‹çš„å±€é™æ€§**ï¼šå¹»è§‰ (Hallucination)ã€åè§ (Bias) ç­‰é—®é¢˜

**æ ¸å¿ƒæ¦‚å¿µï¼šæ³¨æ„åŠ›æœºåˆ¶**

Transformer çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºŽã€Œæ³¨æ„åŠ›å±‚ã€(Attention Layers)ã€‚æ­£å¦‚å¼€åˆ›æ€§è®ºæ–‡ã€ŒAttention Is All You Needã€æ‰€ç¤ºï¼Œæ³¨æ„åŠ›æœºåˆ¶è®©æ¨¡åž‹èƒ½å¤Ÿå…³æ³¨å¥å­ä¸­çš„ç‰¹å®šå•è¯ï¼Œåœ¨å¤„ç†æ¯ä¸ªè¯çš„è¡¨ç¤ºæ—¶é€‚åº¦å¿½ç•¥å…¶ä»–è¯ã€‚

ä¸¾ä¾‹è¯´æ˜Žï¼šåœ¨ç¿»è¯‘ã€ŒYou like this courseã€åˆ°æ³•è¯­æ—¶ï¼Œæ¨¡åž‹éœ€è¦å…³æ³¨ã€ŒYouã€è¿™ä¸ªä¸»è¯­ï¼Œå› ä¸ºæ³•è¯­ä¸­åŠ¨è¯ã€Œlikeã€çš„å˜ä½å–å†³äºŽä¸»è¯­ã€‚

**[å›¾ 3ï¼šæ³¨æ„åŠ›æœºåˆ¶ç¤ºæ„å›¾]**
*è¯´æ˜Žï¼šå±•ç¤ºæ³¨æ„åŠ›å¦‚ä½•åœ¨å¥å­ä¸­çš„ä¸åŒè¯ä¹‹é—´åˆ†é…æƒé‡*

**Chapter 2-4: ä½¿ç”¨ ðŸ¤— Transformers**

- æ¨¡åž‹çš„åŠ è½½ã€ä½¿ç”¨å’Œå¾®è°ƒ
- åœ¨ Hugging Face Hub ä¸Šåˆ†äº«æ¨¡åž‹
- å¤„ç†ä¸åŒä»»åŠ¡çš„å®Œæ•´æµç¨‹

### 3.2 ç¬¬äºŒéƒ¨åˆ†ï¼šæ·±å…¥ NLP å·¥å…·é“¾ (Chapters 5-8)

**Chapter 5-6: æ•°æ®å¤„ç†ä¸Žåˆ†è¯**

- **ðŸ¤— Datasets åº“**ï¼šé«˜æ•ˆåŠ è½½å’Œå¤„ç†å¤§è§„æ¨¡æ•°æ®é›†
- **ðŸ¤— Tokenizers åº“**ï¼šæ·±å…¥ç†è§£åˆ†è¯ (Tokenization) åŽŸç†
  - æ–‡æœ¬å¦‚ä½•è½¬åŒ–ä¸ºæ•°å­—è¡¨ç¤º
  - ä¸åŒåˆ†è¯ç­–ç•¥ (BPE, WordPiece, Unigram)
  - å¦‚ä½•æž„å»ºè‡ªå®šä¹‰åˆ†è¯å™¨

**åˆ†è¯ç¤ºä¾‹**ï¼šåå­—ã€ŒSylvainã€ä¼šè¢«åˆ†è§£ä¸ºå››ä¸ª tokenï¼š`S`, `##yl`, `##va`, `##in`

**Chapter 7-8: ç»å…¸ NLP ä»»åŠ¡ä¸Žå®žç”¨æŠ€èƒ½**

- åºåˆ—åˆ†ç±»ã€å‘½åå®žä½“è¯†åˆ«ã€é—®ç­”ç³»ç»Ÿ
- å¦‚ä½•åœ¨ç¤¾åŒºè®ºå›å¯»æ±‚å¸®åŠ©
- LLM æŽ¨ç†æŠ€æœ¯ä¸Žä¼˜åŒ–

**[å›¾ 4ï¼šæ–‡æœ¬åˆ†è¯è¿‡ç¨‹å¯è§†åŒ–]**
*è¯´æ˜Žï¼šå±•ç¤ºä¸€æ®µæ–‡æœ¬å¦‚ä½•è¢«åˆ†è§£ä¸º tokens*

### 3.3 ç¬¬ä¸‰éƒ¨åˆ†ï¼šLLM å‰æ²¿ä¸»é¢˜ (Chapters 9-12)

è¿™éƒ¨åˆ†æ˜¯è¯¾ç¨‹çš„æ ¸å¿ƒå‡çº§å†…å®¹ï¼Œèšç„¦äºŽå¤§è¯­è¨€æ¨¡åž‹çš„å®žé™…åº”ç”¨ã€‚

**Chapter 9: æž„å»ºå’Œåˆ†äº« Demo**

- ä½¿ç”¨ Gradio åˆ›å»ºäº¤äº’å¼ç•Œé¢
- åœ¨ ðŸ¤— Spaces ä¸Šéƒ¨ç½²æ¨¡åž‹
- å‘ä¸–ç•Œå±•ç¤ºä½ çš„åº”ç”¨

**Chapter 10: ç­–å±•é«˜è´¨é‡æ•°æ®é›†**

æ•°æ®è´¨é‡ç›´æŽ¥å½±å“æ¨¡åž‹æ€§èƒ½ã€‚æœ¬ç« æ•™ä½ ï¼š

- ä¼ ç»ŸæŠ€æœ¯ï¼šåŸºäºŽè§„åˆ™çš„è¿‡æ»¤ã€åŽ»é‡ (ä½¿ç”¨ MinHash æˆ–åµŒå…¥)ã€n-gram åŽ»æ±¡æŸ“
- çŽ°ä»£æ–¹æ³•ï¼šå¥–åŠ±æ¨¡åž‹å’Œè¯„åˆ¤ LLM çš„ç»†ç²’åº¦è´¨é‡æŽ§åˆ¶
- åˆæˆæ•°æ®ç”Ÿæˆå·¥å…·å’Œæ¡†æž¶

**Chapter 11: å¾®è°ƒå¤§è¯­è¨€æ¨¡åž‹**

å¾®è°ƒæ˜¯è®©é¢„è®­ç»ƒæ¨¡åž‹é€‚åº”ç‰¹å®šä»»åŠ¡çš„å…³é”®æŠ€æœ¯ï¼š

- **ç›‘ç£å¾®è°ƒ (Supervised Fine-Tuning, SFT)**ï¼šä½¿ç”¨æŒ‡ä»¤-å›žç­”å¯¹è®­ç»ƒ
- **åå¥½å¯¹é½ (Preference Alignment)**ï¼šä½¿ç”¨é€‰æ‹©/æ‹’ç»å›žç­”è®­ç»ƒ
- **å‚æ•°é«˜æ•ˆå¾®è°ƒ (PEFT)**ï¼šLoRAã€QLoRA ç­‰æŠ€æœ¯é™ä½Žè®¡ç®—æˆæœ¬

**Chapter 12: æž„å»ºæŽ¨ç†æ¨¡åž‹ (æ–°å¢ž)**

è¿™æ˜¯è¯¾ç¨‹æœ€æ–°å¢žåŠ çš„ç« èŠ‚ï¼Œèšç„¦äºŽæž„å»ºå…·æœ‰æŽ¨ç†èƒ½åŠ›çš„æ¨¡åž‹ (å¦‚ DeepSeek R1)ï¼š

- æ€ç»´é“¾ (Chain-of-Thought) æç¤º
- å¤šæ­¥æŽ¨ç†æž¶æž„
- è¯„ä¼°æŽ¨ç†èƒ½åŠ›

**[å›¾ 5ï¼šLLM å¾®è°ƒæµç¨‹å›¾]**
*è¯´æ˜Žï¼šå±•ç¤ºä»Žé¢„è®­ç»ƒæ¨¡åž‹åˆ°ä»»åŠ¡ç‰¹å®šæ¨¡åž‹çš„å¾®è°ƒè¿‡ç¨‹*

## 4. æ ¸å¿ƒæŠ€æœ¯æ·±åº¦è§£æž

### 4.1 Transformer æž¶æž„çš„ä¸‰å¤§å˜ä½“

ç†è§£ä¸åŒæž¶æž„çš„ç‰¹ç‚¹æ˜¯é€‰æ‹©åˆé€‚æ¨¡åž‹çš„å…³é”®ã€‚

**ç¼–ç å™¨æ¨¡åž‹ (Encoder Models)**

- **ä»£è¡¨æ¨¡åž‹**ï¼šBERTã€RoBERTaã€ALBERT
- **ç‰¹ç‚¹**ï¼šåŒå‘æ³¨æ„åŠ›ï¼Œèƒ½çœ‹åˆ°å¥å­ä¸­çš„æ‰€æœ‰è¯
- **é€‚ç”¨ä»»åŠ¡**ï¼šæ–‡æœ¬åˆ†ç±»ã€å‘½åå®žä½“è¯†åˆ«ã€é—®ç­”ç³»ç»Ÿ
- **æ ¸å¿ƒä¼˜åŠ¿**ï¼šæ·±åº¦ç†è§£æ–‡æœ¬è¯­ä¹‰

**è§£ç å™¨æ¨¡åž‹ (Decoder Models)**

- **ä»£è¡¨æ¨¡åž‹**ï¼šGPT ç³»åˆ—ã€Llama ç³»åˆ—
- **ç‰¹ç‚¹**ï¼šå•å‘æ³¨æ„åŠ›ï¼Œåªèƒ½çœ‹åˆ°å‰é¢çš„è¯
- **é€‚ç”¨ä»»åŠ¡**ï¼šæ–‡æœ¬ç”Ÿæˆã€åˆ›æ„å†™ä½œ
- **æ ¸å¿ƒä¼˜åŠ¿**ï¼šæµç•…çš„æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›

**å¤§å¤šæ•°çŽ°ä»£ LLM é‡‡ç”¨çº¯è§£ç å™¨æž¶æž„**ï¼Œè¿™äº›æ¨¡åž‹è§„æ¨¡å·¨å¤§ (æœ‰çš„åŒ…å«æ•°åƒäº¿å‚æ•°)ï¼Œåœ¨ç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€æ–¹é¢è¡¨çŽ°å‡ºè‰²ã€‚

**ç¼–ç å™¨-è§£ç å™¨æ¨¡åž‹ (Encoder-Decoder Models)**

- **ä»£è¡¨æ¨¡åž‹**ï¼šT5ã€BARTã€Whisper (è¯­éŸ³)
- **ç‰¹ç‚¹**ï¼šç»“åˆåŒå‘ç†è§£å’Œå•å‘ç”Ÿæˆ
- **é€‚ç”¨ä»»åŠ¡**ï¼šæœºå™¨ç¿»è¯‘ã€æ–‡æœ¬æ‘˜è¦
- **æ ¸å¿ƒä¼˜åŠ¿**ï¼šåœ¨éœ€è¦ç†è§£è¾“å…¥å¹¶ç”Ÿæˆè¾“å‡ºçš„ä»»åŠ¡ä¸­è¡¨çŽ°ä¼˜å¼‚

**[å›¾ 6ï¼šä¸‰ç§ Transformer æž¶æž„å¯¹æ¯”]**
*è¯´æ˜Žï¼šå¹¶æŽ’å±•ç¤ºä¸‰ç§æž¶æž„çš„ç»“æž„å·®å¼‚å’Œæ•°æ®æµ*

### 4.2 ä»Žé¢„è®­ç»ƒåˆ°å¾®è°ƒï¼šè¿ç§»å­¦ä¹ çš„åŠ›é‡

**é¢„è®­ç»ƒ (Pretraining)**

æƒ³è±¡ä¸€ä¸‹ï¼Œå¦‚æžœæ¯ä¸ªç ”ç©¶å›¢é˜Ÿéƒ½ä»Žå¤´è®­ç»ƒæ¨¡åž‹ï¼Œå°†é€ æˆå·¨å¤§çš„å…¨çƒè®¡ç®—æˆæœ¬å’Œç¢³æŽ’æ”¾ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆå…±äº«è¯­è¨€æ¨¡åž‹è‡³å…³é‡è¦ï¼š

- é¢„è®­ç»ƒæ˜¯ä»Žé›¶å¼€å§‹è®­ç»ƒæ¨¡åž‹ï¼Œæƒé‡éšæœºåˆå§‹åŒ–
- æ¨¡åž‹åœ¨æµ·é‡æ–‡æœ¬æ•°æ®ä¸Šå­¦ä¹ è¯­è¨€çš„ç»Ÿè®¡è§„å¾‹
- è®­ç»ƒæˆæœ¬æžé«˜ï¼Œä½†åªéœ€å®Œæˆä¸€æ¬¡

**å¾®è°ƒ (Fine-tuning)**

- åœ¨é¢„è®­ç»ƒæ¨¡åž‹åŸºç¡€ä¸Šï¼Œé’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œè®­ç»ƒ
- åªéœ€å°‘é‡æ ‡æ³¨æ•°æ®å’Œè®¡ç®—èµ„æº
- èƒ½å¿«é€Ÿé€‚åº”å„ç§ä¸‹æ¸¸ä»»åŠ¡

ä½ å¯ä»¥ä½¿ç”¨ ML CO2 Impact æˆ– Code Carbon (å·²é›†æˆåˆ° ðŸ¤— Transformers) æ¥è¯„ä¼°æ¨¡åž‹è®­ç»ƒçš„ç¢³è¶³è¿¹ã€‚

### 4.3 Pipeline å‡½æ•°ï¼šä¸‰è¡Œä»£ç å®žçŽ° NLP ä»»åŠ¡

ðŸ¤— Transformers çš„ `pipeline()` å‡½æ•°æžå¤§ç®€åŒ–äº†æ¨¡åž‹ä½¿ç”¨ï¼š

```python
from transformers import pipeline

# æ–‡æœ¬ç”Ÿæˆ
generator = pipeline("text-generation", 
                    model="HuggingFaceTB/SmolLM2-360M")
generator("In this course, we will teach you how to",
         max_length=30, num_return_sequences=2)

# é—®ç­”ç³»ç»Ÿ
question_answerer = pipeline("question-answering")
question_answerer(
    question="Where do I work?",
    context="My name is Sylvain and I work at Hugging Face"
)

# é›¶æ ·æœ¬åˆ†ç±»
classifier = pipeline("zero-shot-classification")
classifier(
    "This is a course about the Transformers library",
    candidate_labels=["education", "politics", "business"]
)
```

Pipeline æ”¯æŒå¤šç§æ¨¡æ€ï¼šæ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ï¼Œç”šè‡³å¤šæ¨¡æ€ä»»åŠ¡ã€‚

**[å›¾ 7ï¼šPipeline å·¥ä½œæµç¨‹]**
*è¯´æ˜Žï¼šå±•ç¤ºä»Žè¾“å…¥åˆ°è¾“å‡ºçš„ä¸‰æ­¥å¤„ç†ï¼šé¢„å¤„ç†ã€æ¨¡åž‹æŽ¨ç†ã€åŽå¤„ç†*

## 5. è¯¾ç¨‹ç‰¹è‰²ä¸Žå­¦ä¹ ä½“éªŒ

### 5.1 å®žè·µé©±åŠ¨çš„å­¦ä¹ æ–¹å¼

**äº¤äº’å¼ Notebook**

- æ¯ä¸ªç« èŠ‚éƒ½é…æœ‰ Jupyter Notebook
- æ”¯æŒåœ¨ Google Colab æˆ– Amazon SageMaker Studio Lab ä¸­è¿è¡Œ
- ä»£ç æ‰˜ç®¡åœ¨ [huggingface/notebooks](https://github.com/huggingface/notebooks) ä»“åº“

**åŠ¨æ‰‹é¡¹ç›®**

è¯¾ç¨‹è®ºå›æä¾›ä¸°å¯Œçš„é¡¹ç›®åˆ›æ„ï¼Œå¸®åŠ©ä½ åœ¨å®Œæˆè¯¾ç¨‹åŽç»§ç»­å®žè·µã€‚

### 5.2 å¤šè¯­è¨€æ”¯æŒ

å¾—ç›ŠäºŽå…¨çƒç¤¾åŒºçš„è´¡çŒ®ï¼Œè¯¾ç¨‹å·²è¢«ç¿»è¯‘æˆ 20+ ç§è¯­è¨€ï¼š

- å®Œæ•´ç‰ˆæœ¬ï¼šæ³•è¯­ã€è¶Šå—è¯­ã€ç®€ä½“ä¸­æ–‡
- è¿›è¡Œä¸­ï¼šå¾·è¯­ã€è¥¿ç­ç‰™è¯­ã€æ—¥è¯­ã€éŸ©è¯­ã€è‘¡è„ç‰™è¯­ç­‰

éƒ¨åˆ†è¯­è¨€ç‰ˆæœ¬çš„è¯¾ç¨‹è§†é¢‘è¿˜æä¾›å­—å¹•æ”¯æŒã€‚

**[å›¾ 8ï¼šå…¨çƒå­¦ä¹ è€…åˆ†å¸ƒåœ°å›¾]**
*è¯´æ˜Žï¼šå±•ç¤ºè¯¾ç¨‹åœ¨ä¸åŒå›½å®¶å’Œåœ°åŒºçš„ä½¿ç”¨æƒ…å†µ*

### 5.3 å¼€æºä¸Žåä½œ

**Apache 2.0 è®¸å¯**

- è¯¾ç¨‹å†…å®¹å®Œå…¨å¼€æº
- å¯ä»¥è‡ªç”±ä½¿ç”¨ã€ä¿®æ”¹å’Œåˆ†å‘
- éœ€è¦æ³¨æ˜Žå‡ºå¤„å¹¶æä¾›è®¸å¯è¯é“¾æŽ¥

**ç¤¾åŒºè´¡çŒ®**

åœ¨ [GitHub](https://github.com/huggingface/course) ä¸Šï¼Œä½ å¯ä»¥ï¼š

- æŠ¥å‘Š bug æˆ–æ‹¼å†™é”™è¯¯
- å»ºè®®æ”¹è¿›å†…å®¹
- å‚ä¸Žç¿»è¯‘å·¥ä½œ
- è´¡çŒ®æ–°çš„ç¤ºä¾‹å’Œç»ƒä¹ 

## 6. å­¦ä¹ è·¯å¾„ä¸Žæ—¶é—´æŠ•å…¥

### 6.1 å‰ç½®çŸ¥è¯†è¦æ±‚

- **å¿…éœ€**ï¼šè‰¯å¥½çš„ Python ç¼–ç¨‹åŸºç¡€
- **æŽ¨è**ï¼šå®Œæˆæ·±åº¦å­¦ä¹ å…¥é—¨è¯¾ç¨‹ï¼Œå¦‚ fast.ai çš„ [Practical Deep Learning for Coders](https://course.fast.ai/)
- **å¯é€‰**ï¼šPyTorch æˆ– TensorFlow ç»éªŒä¼šæœ‰å¸®åŠ©ï¼Œä½†ä¸æ˜¯å¿…éœ€çš„

### 6.2 å­¦ä¹ èŠ‚å¥

- æ¯ç« è®¾è®¡ä¸º 1 å‘¨å®Œæˆ
- æ¯å‘¨æŠ•å…¥ 6-8 å°æ—¶
- å¯æ ¹æ®ä¸ªäººèŠ‚å¥çµæ´»è°ƒæ•´

### 6.3 åŽç»­å­¦ä¹ å»ºè®®

å®Œæˆè¯¾ç¨‹åŽï¼ŒæŽ¨èç»§ç»­å­¦ä¹ ï¼š

- DeepLearning.AI çš„ [Natural Language Processing Specialization](https://www.coursera.org/specializations/natural-language-processing)
- Hugging Face çš„å…¶ä»–è¯¾ç¨‹ï¼šAgents Courseã€Diffusion Courseã€Deep RL Course ç­‰

**[å›¾ 9ï¼šå­¦ä¹ è·¯å¾„æŽ¨è]**
*è¯´æ˜Žï¼šå±•ç¤ºä»Žå…¥é—¨åˆ°è¿›é˜¶çš„å®Œæ•´å­¦ä¹ è·¯å¾„*

## 7. è¯¾ç¨‹å›¢é˜Ÿï¼šé¡¶å°–ä¸“å®¶å€¾åŠ›æ‰“é€ 

### 7.1 æ ¸å¿ƒä½œè€…å›¢é˜Ÿ

**Abubakar Abid**ï¼šæ–¯å¦ç¦å¤§å­¦åº”ç”¨æœºå™¨å­¦ä¹ åšå£«ï¼ŒGradio åˆ›å§‹äººã€‚Gradio å·²è¢«ç”¨äºŽæž„å»ºè¶…è¿‡ 60 ä¸‡ä¸ªæœºå™¨å­¦ä¹ æ¼”ç¤ºï¼ŒåŽè¢« Hugging Face æ”¶è´­ã€‚

**Sylvain Gugger**ï¼šðŸ¤— Transformers æ ¸å¿ƒç»´æŠ¤è€…ä¹‹ä¸€ï¼Œæ›¾ä»» fast.ai ç ”ç©¶ç§‘å­¦å®¶ï¼Œä¸Ž Jeremy Howard åˆè‘—ã€ŠDeep Learning for Coders with fastai and PyTorchã€‹ã€‚ç ”ç©¶é‡ç‚¹æ˜¯åœ¨æœ‰é™èµ„æºä¸‹è®­ç»ƒæ¨¡åž‹ã€‚

**Lewis Tunstall å’Œ Leandro von Werra**ï¼šO'Reilly å›¾ä¹¦ã€ŠNatural Language Processing with Transformersã€‹å…±åŒä½œè€…ï¼Œåœ¨å°† NLP é¡¹ç›®æŠ•å…¥ç”Ÿäº§æ–¹é¢ç»éªŒä¸°å¯Œã€‚

### 7.2 å…¶ä»–æ ¸å¿ƒè´¡çŒ®è€…

- **Ben Burtenshaw**ï¼šå®‰ç‰¹å«æ™®å¤§å­¦ NLP åšå£«ï¼Œä¸“æ³¨æ•™è‚²ææ–™å’Œç¤¾åŒºå·¥å…·
- **Matthew Carrigan**ã€**Lysandre Debut**ï¼šðŸ¤— Transformers åº“æ—©æœŸå¼€å‘è€…
- **Merve Noyan**ã€**Lucile Saulnier**ï¼šå¼€æºå·¥å…·å¼€å‘ä¸Žæ”¯æŒ

## 8. å®žé™…åº”ç”¨ï¼šLLM æ­£åœ¨æ”¹å˜ä»€ä¹ˆï¼Ÿ

### 8.1 äº§ä¸šåº”ç”¨åœºæ™¯

**å†…å®¹åˆ›ä½œ**

- æ–‡ç« å†™ä½œè¾…åŠ©
- ä»£ç ç”Ÿæˆ (GitHub Copilot)
- åˆ›æ„æ•…äº‹ç”Ÿæˆ

**çŸ¥è¯†å·¥ä½œ**

- æ™ºèƒ½é—®ç­”ç³»ç»Ÿ
- æ–‡æ¡£æ‘˜è¦ä¸Žåˆ†æž
- å¤šè¯­è¨€ç¿»è¯‘

**å®¢æˆ·æœåŠ¡**

- æ™ºèƒ½å®¢æœæœºå™¨äºº
- æƒ…æ„Ÿåˆ†æž
- ä¸ªæ€§åŒ–æŽ¨è

### 8.2 ç ”ç©¶ä¸Žå¼€å‘

- æ–°æ¨¡åž‹æž¶æž„æŽ¢ç´¢
- é«˜æ•ˆè®­ç»ƒæŠ€æœ¯ç ”ç©¶
- å¤šæ¨¡æ€æ¨¡åž‹å¼€å‘

**[å›¾ 10ï¼šLLM åº”ç”¨ç”Ÿæ€ç³»ç»Ÿ]**
*è¯´æ˜Žï¼šå±•ç¤º LLM åœ¨ä¸åŒé¢†åŸŸçš„åº”ç”¨æ¡ˆä¾‹*

## 9. å…³é”®æŠ€æœ¯è¶‹åŠ¿

### 9.1 æ¨¡åž‹è§„æ¨¡ä¸Žæ•ˆçŽ‡çš„å¹³è¡¡

çŽ°ä»£ LLM å‘å±•å‘ˆçŽ°ä¸¤ä¸ªæ–¹å‘ï¼š

- **è§„æ¨¡åŒ–**ï¼šGPT-4ã€Llama 3.1 ç­‰è¶…å¤§è§„æ¨¡æ¨¡åž‹
- **æ•ˆçŽ‡åŒ–**ï¼šSmolLMã€Phi ç­‰å°åž‹é«˜æ•ˆæ¨¡åž‹

è¯¾ç¨‹æ¶µç›–ä¸¤ä¸ªæ–¹å‘çš„æŠ€æœ¯ç»†èŠ‚ã€‚

### 9.2 æŽ¨ç†ä¼˜åŒ–æŠ€æœ¯

- **Flash Attention**ï¼šå°†æ³¨æ„åŠ›æœºåˆ¶å¤æ‚åº¦ä»Žå¹³æ–¹é™åˆ°çº¿æ€§
- **Key-Value Cache**ï¼šMQA å’Œ GQA æ”¹è¿›
- **æŠ•æœºè§£ç  (Speculative Decoding)**ï¼šä½¿ç”¨å°æ¨¡åž‹ç”Ÿæˆè‰ç¨¿ï¼Œå¤§æ¨¡åž‹å®¡æ ¸

### 9.3 æ•°æ®è´¨é‡çš„é‡è¦æ€§

é«˜è´¨é‡æ•°æ®é›†æ˜¯è®­ç»ƒä¼˜ç§€æ¨¡åž‹çš„åŸºç¡€ï¼š

- **FineWeb**ï¼š15T è§„æ¨¡çš„é¢„è®­ç»ƒæ•°æ®é›†
- **RedPajama v2**ï¼šå¸¦æœ‰è´¨é‡è¿‡æ»¤çš„å¤§è§„æ¨¡æ•°æ®
- **åˆæˆæ•°æ®**ï¼šä½¿ç”¨ Distilabel ç­‰å·¥å…·ç”Ÿæˆ

## 10. ç»“è¯­

Hugging Face çš„ LLM Course æ˜¯ä¸€é—¨ç³»ç»Ÿã€å®žç”¨ã€ä¸Žæ—¶ä¿±è¿›çš„å…è´¹è¯¾ç¨‹ã€‚ä»Ž Transformer åŸºç¡€åˆ° LLM å¾®è°ƒï¼Œä»Žæ•°æ®å¤„ç†åˆ°æ¨¡åž‹éƒ¨ç½²ï¼Œè¯¾ç¨‹è¦†ç›–äº†çŽ°ä»£ NLP å’Œ LLM å¼€å‘çš„å®Œæ•´æŠ€èƒ½æ ‘ã€‚

æ— è®ºä½ æ˜¯æƒ³äº†è§£ AI å‰æ²¿æŠ€æœ¯çš„ç ”ç©¶è€…ï¼Œè¿˜æ˜¯å¸Œæœ›å°† LLM åº”ç”¨äºŽå®žé™…é¡¹ç›®çš„å·¥ç¨‹å¸ˆï¼Œè¿™é—¨è¯¾ç¨‹éƒ½èƒ½ä¸ºä½ æä¾›åšå®žçš„ç†è®ºåŸºç¡€å’Œä¸°å¯Œçš„å®žè·µç»éªŒã€‚

ç«‹å³è®¿é—® [è¯¾ç¨‹ä¸»é¡µ](https://huggingface.co/learn/llm-course) å¼€å§‹å­¦ä¹ ï¼ä¸Žå…¨çƒå¼€å‘è€…ä¸€èµ·ï¼ŒæŽ¢ç´¢å¤§è¯­è¨€æ¨¡åž‹çš„æ— é™å¯èƒ½ã€‚

---

**ç›¸å…³èµ„æº**ï¼š

- **è¯¾ç¨‹ä¸»é¡µ**ï¼šhttps://huggingface.co/learn/llm-course
- **GitHub ä»“åº“**ï¼šhttps://github.com/huggingface/course
- **Notebook ä»“åº“**ï¼šhttps://github.com/huggingface/notebooks
- **ç¤¾åŒºè®ºå›**ï¼šhttps://discuss.huggingface.co
- **YouTube è§†é¢‘**ï¼šhttps://youtube.com/playlist?list=PLo2EIpI_JMQvWfQndUesu0nPBAtZ9gP1o

---

**æ³¨**ï¼šæœ¬æ–‡æ‰€éœ€å›¾ç‰‡æ¸…å•

1. **å›¾ 1**ï¼šLLM Course è¯¾ç¨‹æž¶æž„å›¾ (å±•ç¤º 12 ä¸ªç« èŠ‚çš„ç»“æž„)
2. **å›¾ 2**ï¼šNLP åˆ° LLM çš„æŠ€æœ¯æ¼”è¿› (æ—¶é—´çº¿å±•ç¤º)
3. **å›¾ 3**ï¼šæ³¨æ„åŠ›æœºåˆ¶ç¤ºæ„å›¾ (è¯ä¸Žè¯ä¹‹é—´çš„å…³è”)
4. **å›¾ 4**ï¼šæ–‡æœ¬åˆ†è¯è¿‡ç¨‹å¯è§†åŒ– (Token åˆ†è§£ç¤ºä¾‹)
5. **å›¾ 5**ï¼šLLM å¾®è°ƒæµç¨‹å›¾ (é¢„è®­ç»ƒåˆ°å¾®è°ƒçš„æµç¨‹)
6. **å›¾ 6**ï¼šä¸‰ç§ Transformer æž¶æž„å¯¹æ¯” (å¹¶æŽ’ç»“æž„å›¾)
7. **å›¾ 7**ï¼šPipeline å·¥ä½œæµç¨‹ (ä¸‰æ­¥å¤„ç†æµç¨‹)
8. **å›¾ 8**ï¼šå…¨çƒå­¦ä¹ è€…åˆ†å¸ƒåœ°å›¾ (ä¸–ç•Œåœ°å›¾æ ‡è®°)
9. **å›¾ 9**ï¼šå­¦ä¹ è·¯å¾„æŽ¨è (è¿›é˜¶è·¯å¾„å›¾)
10. **å›¾ 10**ï¼šLLM åº”ç”¨ç”Ÿæ€ç³»ç»Ÿ (åº”ç”¨åœºæ™¯åˆé›†)

---

> è¿žäº«ä¼šï¼šä¸“æ³¨äºŽç»æµŽå­¦ã€ç»Ÿè®¡å­¦å’Œæ•°æ®ç§‘å­¦çš„å­¦æœ¯äº¤æµä¸ŽçŸ¥è¯†ä¼ æ’­
> 
> ç½‘ç«™ï¼šhttps://www.lianxh.cn
